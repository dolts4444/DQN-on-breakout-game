{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils_drl import Agent\n",
    "from utils_env import MyEnv\n",
    "from utils_memory import ReplayMemory, PRMemory\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "use_dueling = False\n",
    "use_PR = True\n",
    "use_DDQN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "GLOBAL_SEED = 0\n",
    "MEM_SIZE = 100_000\n",
    "RENDER = False\n",
    "SAVE_PREFIX = \"./models\"\n",
    "STACK_SIZE = 4\n",
    "\n",
    "EPS_START = 1.\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "POLICY_UPDATE = 4\n",
    "TARGET_UPDATE = 10_000\n",
    "WARM_STEPS = 50_000\n",
    "MAX_STEPS = 50_000_000\n",
    "EVALUATE_FREQ = 100_000\n",
    "\n",
    "rand = random.Random()\n",
    "rand.seed(GLOBAL_SEED)\n",
    "new_seed = lambda: rand.randint(0, 1000_000)\n",
    "if not os.path.exists(SAVE_PREFIX):\n",
    "    os.mkdir(SAVE_PREFIX)\n",
    "\n",
    "torch.manual_seed(new_seed())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = MyEnv(device)\n",
    "agent = Agent(\n",
    "    env.get_action_dim(),\n",
    "    device,\n",
    "    GAMMA,\n",
    "    new_seed(),\n",
    "    EPS_START,\n",
    "    EPS_END,\n",
    "    EPS_DECAY,\n",
    "    use_dueling = use_dueling,\n",
    "    use_PR = use_PR\n",
    ")\n",
    "if not use_PR:\n",
    "    memory = ReplayMemory(STACK_SIZE + 1, MEM_SIZE, device)\n",
    "else:\n",
    "    memory = PRMemory(STACK_SIZE + 1, MEM_SIZE, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%| | 409539/50000000 [1:55:10<266:13:12, 51.74b/s] "
     ]
    }
   ],
   "source": [
    "#### Training ####\n",
    "obs_queue: deque = deque(maxlen=5)\n",
    "done = True\n",
    "\n",
    "progressive = tqdm(range(MAX_STEPS), total=MAX_STEPS,\n",
    "                   ncols=50, leave=False, unit=\"b\")\n",
    "for step in progressive:\n",
    "    if done:\n",
    "        observations, _, _ = env.reset()\n",
    "        for obs in observations:\n",
    "            obs_queue.append(obs)\n",
    "\n",
    "    training = len(memory) > WARM_STEPS\n",
    "    state = env.make_state(obs_queue).to(device).float()\n",
    "    action = agent.run(state, training)\n",
    "    obs, reward, done = env.step(action)\n",
    "    obs_queue.append(obs)\n",
    "    memory.push(env.make_folded_state(obs_queue), action, reward, done)\n",
    "\n",
    "    if step % POLICY_UPDATE == 0 and training:\n",
    "        if not use_PR:\n",
    "            agent.learn(memory, BATCH_SIZE)\n",
    "        else:\n",
    "            agent.learn_PR(memory, BATCH_SIZE)\n",
    "\n",
    "    if step % TARGET_UPDATE == 0:\n",
    "        agent.sync()\n",
    "\n",
    "    if step % EVALUATE_FREQ == 0:\n",
    "        avg_reward, frames = env.evaluate(obs_queue, agent, render=RENDER)\n",
    "        with open(\"rewards.txt\", \"a\") as fp:\n",
    "            fp.write(f\"{step//EVALUATE_FREQ:3d} {step:8d} {avg_reward:.1f}\\n\")\n",
    "        if RENDER:\n",
    "            prefix = f\"eval_{step//EVALUATE_FREQ:03d}\"\n",
    "            os.mkdir(prefix)\n",
    "            for ind, frame in enumerate(frames):\n",
    "                with open(os.path.join(prefix, f\"{ind:06d}.png\"), \"wb\") as fp:\n",
    "                    frame.save(fp, format=\"png\")\n",
    "        agent.save(os.path.join(\n",
    "            SAVE_PREFIX, f\"model_{step//EVALUATE_FREQ:03d}\"))\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute '__policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8b8014ba8113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mISWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalues_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__gamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute '__policy'"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "state_batch, action_batch, reward_batch, next_batch, done_batch, idxs, ISWeights = memory.sample(batch_size)\n",
    "\n",
    "values = agent.__policy(state_batch.float()).gather(1, action_batch)\n",
    "values_next = agent.__target(next_batch.float()).max(1).values.detach()\n",
    "expected = (agent.__gamma * values_next.unsqueeze(1)) * (1. - done_batch) + reward_batch\n",
    "            \n",
    "abs_errors = torch.abs(expected - values).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010],\n",
       "        [0.0010],\n",
       "        [0.0010],\n",
       "        [0.0010],\n",
       "        [0.0067],\n",
       "        [0.0011],\n",
       "        [0.0120],\n",
       "        [0.0046],\n",
       "        [0.0053],\n",
       "        [0.0088],\n",
       "        [0.0058],\n",
       "        [0.0086],\n",
       "        [0.0025],\n",
       "        [0.0036],\n",
       "        [0.0086],\n",
       "        [0.0042],\n",
       "        [0.0048],\n",
       "        [0.0221],\n",
       "        [0.0037],\n",
       "        [0.0034],\n",
       "        [0.0042],\n",
       "        [0.0024],\n",
       "        [0.0032],\n",
       "        [0.0269],\n",
       "        [0.0027],\n",
       "        [0.0090],\n",
       "        [0.0013],\n",
       "        [0.0067],\n",
       "        [0.0022],\n",
       "        [0.0051],\n",
       "        [0.0019],\n",
       "        [0.0010]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ISWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001007135258987546"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.tree.get_min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187.83919137775848"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/MEM_SIZE/min_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3888310072684988 0.13621489703655243 tensor(0.0074, device='cuda:0')\n",
      "0.18917947265625 1.0 tensor(0.0010, device='cuda:0')\n",
      "0.18917947265625 1.0 tensor(0.0010, device='cuda:0')\n",
      "0.18917947265625 1.0 tensor(0.0010, device='cuda:0')\n",
      "0.694941048779361 0.2722237706184387 tensor(0.0037, device='cuda:0')\n",
      "0.37210651845112763 0.5084013938903809 tensor(0.0020, device='cuda:0')\n",
      "0.26256263146269637 0.720511794090271 tensor(0.0014, device='cuda:0')\n",
      "1.749586569519624 0.10812810063362122 tensor(0.0093, device='cuda:0')\n",
      "3.3081405345428454 0.05718604475259781 tensor(0.0176, device='cuda:0')\n",
      "1.1463828804292961 0.16502293944358826 tensor(0.0061, device='cuda:0')\n",
      "1.506545290816693 0.12557171285152435 tensor(0.0080, device='cuda:0')\n",
      "0.7791874310698271 0.24279071390628815 tensor(0.0041, device='cuda:0')\n",
      "0.6026656460260472 0.3139045238494873 tensor(0.0032, device='cuda:0')\n",
      "0.3081877845731952 0.6138448119163513 tensor(0.0016, device='cuda:0')\n",
      "0.6099925844511421 0.31013405323028564 tensor(0.0032, device='cuda:0')\n",
      "0.8108303188403502 0.23331573605537415 tensor(0.0043, device='cuda:0')\n",
      "0.49764019528024145 0.3801531195640564 tensor(0.0026, device='cuda:0')\n",
      "0.7812499020534578 0.24214975535869598 tensor(0.0042, device='cuda:0')\n",
      "1.5194126463633415 0.12450829148292542 tensor(0.0081, device='cuda:0')\n",
      "0.9063157597143279 0.20873461663722992 tensor(0.0048, device='cuda:0')\n",
      "1.6702094562894823 0.11326691508293152 tensor(0.0089, device='cuda:0')\n",
      "0.7778701027106055 0.24320188164710999 tensor(0.0041, device='cuda:0')\n",
      "5.246384996476064 0.03605901449918747 tensor(0.0279, device='cuda:0')\n",
      "0.6124306256367014 0.3088994324207306 tensor(0.0033, device='cuda:0')\n",
      "1.0857107344297265 0.17424482107162476 tensor(0.0058, device='cuda:0')\n",
      "0.7852568374057692 0.24091413617134094 tensor(0.0042, device='cuda:0')\n",
      "0.6345756898683276 0.2981196343898773 tensor(0.0034, device='cuda:0')\n",
      "0.5435285626089947 0.3480580151081085 tensor(0.0029, device='cuda:0')\n",
      "1.151204956901969 0.16433170437812805 tensor(0.0061, device='cuda:0')\n",
      "0.9398521392737381 0.2012864202260971 tensor(0.0050, device='cuda:0')\n",
      "0.9645234147492177 0.19613777101039886 tensor(0.0051, device='cuda:0')\n",
      "0.18917947265625 1.0 tensor(0.0010, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n=32\n",
    "pri_seg = memory.tree.total_p / n       # priority segment\n",
    "memory.beta = np.min([1., memory.beta + memory.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "min_prob = memory.tree.get_min() / memory.tree.total_p     # for later calculate ISweight\n",
    "if min_prob == 0:\n",
    "    min_prob = 0.00001 / memory.tree.total_p\n",
    "for i in range(n):\n",
    "    a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "    v = np.random.uniform(a, b)\n",
    "    idx, p, data = memory.tree.get_leaf(v)\n",
    "    prob = p / memory.tree.total_p\n",
    "    ISWeights[i, 0] = np.power(prob/min_prob, -memory.beta)\n",
    "    #print(b_states[i].shape, data[0][0:4].shape)\n",
    "    print(1/prob/MEM_SIZE , p ,ISWeights[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(memory.tree.tree[-MEM_SIZE:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
